# ParallelDots SAM2 Object Tracking Assignment

## Overview

This notebook implements object tracking and segmentation using Meta's Segment Anything Model 2 (SAM2). It addresses three assignment problem statements:

1. Generating bounding boxes from ground truth masks.
2. Tracking objects across video frames using SAM2.
3. Evaluating performance using IoU and COCO metrics.

## Requirements

* **Environment:** Google Colab with GPU Runtime.
* **Dependencies:** PyTorch, Hydra-core, Detectron2, SAM2.

## Setup

1. **Install Dependencies:** Execute the initial cells to install libraries and clone the SAM2 repository.
2. **Upload Files:** The notebook prompts for the following files:
* `sam2.1_hiera_tiny.pt` (Model weights)
* `sam2.1_hiera_t.yaml` (Model configuration)
* `data_2D.zip` (Dataset)



## Workflow

### 1. Model Initialization

The notebook initializes the SAM2-Hiera-Tiny model and sets up the `SAM2VideoPredictor` for tracking tasks.

### 2. Processing & Tracking

The `process_and_evaluate_dataset` function iterates through the dataset to:

* Extract initial bounding boxes from masks using `process_img_mask_pair` (adds 10% padding).
* Propagate masks to subsequent frames using `track_item_boxes`.
* Save tracking results to CSV files.

### 3. Evaluation

The `evaluate_coco_performance_with_iou` function calculates and displays:

* Average IoU per product.
* standard COCO metrics (mAP 50:95, AP50, AP75, Recall).

## Outputs

* **predictions.csv:** Bounding box predictions generated by the model.
* **ground_truth.csv:** Ground truth bounding boxes derived from validation masks.
* **Console:** A performance table summarizing metrics for each product class (e.g., `can_chowder`, `diet_coke`).